{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1xnOCF5EBjDDapoE7LJOccB0nM1yz4Ir5",
      "authorship_tag": "ABX9TyN2GS89SEInVqE5/tuWgPXV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caaszj/GLAFormer/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# 数据预处理和加载\n",
        "class HSIChangeDetectionDataset(Dataset):\n",
        "    def __init__(self, before_path, after_path, gt_path, patch_size=9, mode='train'):\n",
        "        self.before = self.load_mat(before_path)  # (H, W, C)\n",
        "        self.after = self.load_mat(after_path)\n",
        "        self.gt = self.load_gt(gt_path)  # (H, W)\n",
        "\n",
        "        # 检查标签值\n",
        "        unique_labels = np.unique(self.gt)\n",
        "        print(f\"标签中的唯一值: {unique_labels}\")\n",
        "\n",
        "        # 归一化\n",
        "        self.before = self._normalize(self.before)\n",
        "        self.after = self._normalize(self.after)\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.half = patch_size // 2\n",
        "        self.mode = mode\n",
        "\n",
        "        # 生成有效位置索引\n",
        "        self.coords = self.get_valid_coords()\n",
        "\n",
        "        # 计算类别分布\n",
        "        self.class_counts = self._get_class_distribution()\n",
        "        print(f\"类别分布 - 未变化: {self.class_counts[0]}, 变化: {self.class_counts[1]}\")\n",
        "\n",
        "        # 划分训练/验证/测试集\n",
        "        np.random.seed(42)  # 设置随机种子以确保可重复性\n",
        "        idx = np.random.permutation(len(self.coords))  # 随机打乱索引\n",
        "        train_num = int(len(idx) * 0.7)  # 训练集占70%\n",
        "        val_num = int(len(idx) * 0.15)  # 验证集占15%\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.coords = self.coords[idx[:train_num]]  # 取前70%作为训练集\n",
        "        elif mode == 'val':\n",
        "            self.coords = self.coords[idx[train_num:train_num + val_num]]  # 取接下来的15%作为验证集\n",
        "        else:\n",
        "            self.coords = self.coords[idx[train_num + val_num:]]  # 剩下的15%作为测试集\n",
        "\n",
        "    def _normalize(self, data):\n",
        "        \"\"\"安全的归一化函数，处理可能的零除问题\"\"\"\n",
        "        data_min = data.min()\n",
        "        data_max = data.max()\n",
        "        if data_max == data_min:\n",
        "            return np.zeros_like(data)\n",
        "        return (data - data_min) / (data_max - data_min)\n",
        "\n",
        "    def _get_class_distribution(self):\n",
        "        \"\"\"计算变化/非变化类别的样本数量\"\"\"\n",
        "        class_counts = [0, 0]\n",
        "        for i, j in self.coords:\n",
        "            label = 1 if self.gt[i, j] == 255 else 0\n",
        "            class_counts[label] += 1\n",
        "        return class_counts\n",
        "\n",
        "    def load_mat(self, path):\n",
        "        mat = loadmat(path)\n",
        "        # 获取.mat文件中的第一个键，假设它是数据键\n",
        "        keys = [k for k in mat.keys() if not k.startswith('__')]\n",
        "        if 'river_before' in mat:\n",
        "            return mat['river_before'].astype(np.float32)\n",
        "        elif 'river_after' in mat:\n",
        "            return mat['river_after'].astype(np.float32)\n",
        "        elif len(keys) > 0:\n",
        "            return mat[keys[0]].astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"无法从{path}加载数据\")\n",
        "\n",
        "    def load_gt(self, path):\n",
        "        mat = loadmat(path)\n",
        "        keys = [k for k in mat.keys() if not k.startswith('__')]\n",
        "        if 'lakelabel_v1' in mat:\n",
        "            return mat['lakelabel_v1'].astype(np.int64)\n",
        "        elif len(keys) > 0:\n",
        "            return mat[keys[0]].astype(np.int64)\n",
        "        else:\n",
        "            raise ValueError(f\"无法从{path}加载标签\")\n",
        "\n",
        "    def get_valid_coords(self):\n",
        "        H, W = self.gt.shape\n",
        "        coords = []\n",
        "        for i in range(self.half, H-self.half):\n",
        "            for j in range(self.half, W-self.half):\n",
        "                if self.gt[i, j] == 0 or self.gt[i, j] == 255:  # 只处理有效像素\n",
        "                    coords.append((i, j))\n",
        "        return np.array(coords)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, j = self.coords[idx]\n",
        "        # 提取双时相patch\n",
        "        before_patch = self.before[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        after_patch = self.after[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        # 转为CHW格式\n",
        "        before_patch = torch.from_numpy(before_patch).permute(2,0,1).float()\n",
        "        after_patch = torch.from_numpy(after_patch).permute(2,0,1).float()\n",
        "        label = 1 if self.gt[i, j] == 255 else 0  # 确保标签为0或1\n",
        "\n",
        "        return before_patch, after_patch, label\n",
        "\n",
        "# GLAM模块实现\n",
        "class GLAM(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, window_size=3):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # 确保维度可以被头数整除\n",
        "        self.split_dim = dim // 2\n",
        "        self.num_heads_local = max(1, num_heads // 2)  # 至少1个头\n",
        "        # 确保head_dim_local是整数\n",
        "        self.head_dim_local = self.split_dim // self.num_heads_local\n",
        "        self.split_dim = self.head_dim_local * self.num_heads_local  # 重新计算以确保可整除\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Local分支\n",
        "        self.local_qkv = nn.Conv2d(self.split_dim, self.split_dim*3, kernel_size=1)\n",
        "\n",
        "        # Global分支\n",
        "        self.global_q = nn.Conv2d(self.split_dim, self.split_dim, kernel_size=1)\n",
        "        self.global_kv = nn.Conv2d(self.split_dim, self.split_dim*2, kernel_size=1)\n",
        "\n",
        "        # 确保投影层维度正确\n",
        "        self.proj = nn.Conv2d(self.split_dim*2, dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # 分割通道维度\n",
        "        x_split = torch.split(x, [self.split_dim, C - self.split_dim], dim=1)\n",
        "        x_local = x_split[0]\n",
        "        x_global = x_split[0] if len(x_split) == 1 else x_split[1]\n",
        "\n",
        "        # 保证x_global的维度与split_dim一致\n",
        "        if x_global.shape[1] != self.split_dim:\n",
        "            x_global = F.adaptive_avg_pool2d(x_global, (H, W))\n",
        "            x_global = F.interpolate(x_global, size=(H, W), mode='bilinear')\n",
        "            if x_global.shape[1] > self.split_dim:\n",
        "                x_global = x_global[:, :self.split_dim, :, :]\n",
        "            elif x_global.shape[1] < self.split_dim:\n",
        "                padding = self.split_dim - x_global.shape[1]\n",
        "                x_global = torch.cat([x_global, torch.zeros(B, padding, H, W, device=x.device)], dim=1)\n",
        "\n",
        "        # Local Attention\n",
        "        qkv = self.local_qkv(x_local)  # [B, 3*split_dim, H, W]\n",
        "        qkv = qkv.reshape(B, 3, self.num_heads_local, self.head_dim_local, H, W)\n",
        "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # [B, num_heads_local, head_dim_local, H, W]\n",
        "\n",
        "        # 重塑张量以计算注意力\n",
        "        q = q.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k = k.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        v = v.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "\n",
        "        # 计算注意力分数 - 修正维度顺序\n",
        "        attn = torch.matmul(q.transpose(-2, -1), k) / math.sqrt(self.head_dim_local)  # [B, num_heads_local, H*W, H*W]\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        # 应用注意力\n",
        "        local_out = torch.matmul(v, attn.transpose(-2, -1))  # [B, num_heads_local, head_dim_local, H*W]\n",
        "        local_out = local_out.reshape(B, self.split_dim, H, W)\n",
        "\n",
        "        # Global Attention\n",
        "        x_pool = F.avg_pool2d(x_global, self.window_size)\n",
        "        pool_H, pool_W = x_pool.shape[2], x_pool.shape[3]\n",
        "\n",
        "        kv = self.global_kv(x_pool)  # [B, 2*split_dim, pool_H, pool_W]\n",
        "        kv = kv.reshape(B, 2, self.num_heads_local, self.head_dim_local, pool_H, pool_W)\n",
        "        k_g, v_g = kv[:, 0], kv[:, 1]  # [B, num_heads_local, head_dim_local, pool_H, pool_W]\n",
        "\n",
        "        q_g = self.global_q(x_global)  # [B, split_dim, H, W]\n",
        "        q_g = q_g.reshape(B, self.num_heads_local, self.head_dim_local, H, W)\n",
        "\n",
        "        # 重塑张量以计算注意力\n",
        "        q_g = q_g.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k_g = k_g.reshape(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "        v_g = v_g.reshape(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_g = torch.matmul(q_g.transpose(-2, -1), k_g) / math.sqrt(self.head_dim_local)  # [B, num_heads_local, H*W, pool_H*pool_W]\n",
        "        attn_g = F.softmax(attn_g, dim=-1)\n",
        "\n",
        "        # 应用注意力\n",
        "        global_out = torch.matmul(v_g, attn_g.transpose(-2, -1))  # [B, num_heads_local, head_dim_local, H*W]\n",
        "        global_out = global_out.reshape(B, self.split_dim, H, W)\n",
        "\n",
        "        # 连接并投影\n",
        "        out = torch.cat([local_out, global_out], dim=1)\n",
        "        return self.proj(out)\n",
        "\n",
        "# CGFN模块实现\n",
        "class CGFN(nn.Module):\n",
        "    def __init__(self, dim, expansion=4):\n",
        "        super().__init__()\n",
        "        hidden_dim = dim * expansion\n",
        "\n",
        "        self.conv1 = nn.Conv2d(dim, hidden_dim, 1)\n",
        "        self.dwconv3 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, groups=hidden_dim)\n",
        "        self.dwconv5 = nn.Conv2d(hidden_dim, hidden_dim, 5, padding=2, groups=hidden_dim)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim*2, dim, 1)\n",
        "\n",
        "        self.gate1 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gate2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.dwconv3(x)\n",
        "        x2 = self.dwconv5(x)\n",
        "\n",
        "        g1 = self.gate1(x1)\n",
        "        g2 = self.gate2(x2)\n",
        "\n",
        "        x1 = x1 * g1 + x2\n",
        "        x2 = x2 * g2 + x1\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        return self.conv2(x)\n",
        "\n",
        "# 辅助模块\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super().__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# GLAFormer Block\n",
        "class GLAFormerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size=3):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.glam = GLAM(dim, num_heads, window_size)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.cgfn = CGFN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 注意这里使用LayerNorm，需要先调整维度(B,C,H,W)->(B,H,W,C)\n",
        "        B, C, H, W = x.shape\n",
        "        x_perm = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)  # 归一化后转回[B, C, H, W]\n",
        "        x = x + self.glam(x_norm)\n",
        "\n",
        "        x_perm = x.permute(0, 2, 3, 1)\n",
        "        x_norm = self.norm2(x_perm).permute(0, 3, 1, 2)\n",
        "        x = x + self.cgfn(x_norm)\n",
        "        return x\n",
        "\n",
        "# 完整模型\n",
        "class GLAFormer(nn.Module):\n",
        "    def __init__(self, in_channels=198, dim=256, num_blocks=4, num_heads=8, patch_size=9):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.dim = dim\n",
        "\n",
        "        # 动态计算中心位置，避免硬编码\n",
        "        self.register_buffer('center', torch.tensor(patch_size // 2, dtype=torch.long))\n",
        "\n",
        "        # 输入嵌入层\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, dim, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 主干网络\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[GLAFormerBlock(dim, num_heads) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        # 分类头\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(dim, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 2, kernel_size=1)  # 输出通道数为2\n",
        "        )\n",
        "\n",
        "        # 全局特征和中心特征提取\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # 检查输入\n",
        "        if x1.dim() != 4 or x2.dim() != 4:\n",
        "            raise ValueError(f\"输入维度不正确: x1={x1.shape}, x2={x2.shape}\")\n",
        "\n",
        "        # 双时相特征融合\n",
        "        x = torch.cat([x1, x2], dim=1)  # [B, 2*in_channels, patch_size, patch_size]\n",
        "        x = self.embed(x)               # [B, dim, patch_size, patch_size]\n",
        "        x = self.blocks(x)              # [B, dim, patch_size, patch_size]\n",
        "        x = self.conv_block(x)          # [B, 2, patch_size, patch_size]\n",
        "\n",
        "        # 提取全局特征\n",
        "        global_feat = self.global_pool(x).flatten(1)  # [B, 2]\n",
        "\n",
        "        # 提取中心特征 - 使用动态计算的中心位置\n",
        "        center = self.center.item()\n",
        "        center_feat = x[:, :, center, center]  # [B, 2]\n",
        "\n",
        "        # 特征融合（加权平均）\n",
        "        return 0.6 * global_feat + 0.4 * center_feat  # [B, 2]\n",
        "\n",
        "# 计算混淆矩阵和详细指标\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"计算混淆矩阵和详细性能指标\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # 计算各种指标\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # 计算Kappa系数\n",
        "    pe = ((tn + fp) * (tn + fn) + (fn + tp) * (fp + tp)) / ((tp + tn + fp + fn) ** 2)\n",
        "    kappa = (accuracy - pe) / (1 - pe) if (1 - pe) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'confusion_matrix': cm,\n",
        "        'accuracy': accuracy * 100,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'kappa': kappa\n",
        "    }\n",
        "\n",
        "# 评估函数\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for before_patch, after_patch, labels in data_loader:\n",
        "            before_patch = before_patch.to(device)\n",
        "            after_patch = after_patch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(before_patch, after_patch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * before_patch.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(data_loader.dataset)\n",
        "    metrics = compute_metrics(all_labels, all_preds)\n",
        "\n",
        "    return val_loss, metrics\n",
        "\n",
        "# 学习率预热和余弦退火调度器\n",
        "class WarmupCosineScheduler:\n",
        "    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.min_lr = min_lr\n",
        "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
        "\n",
        "    def step(self, epoch):\n",
        "        if epoch < self.warmup_epochs:\n",
        "            # 线性预热\n",
        "            lr_scale = epoch / self.warmup_epochs\n",
        "        else:\n",
        "            # 余弦退火\n",
        "            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
        "            lr_scale = max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "        for i, group in enumerate(self.optimizer.param_groups):\n",
        "            group['lr'] = self.base_lrs[i] * lr_scale + self.min_lr\n",
        "\n",
        "def main():\n",
        "    # 设置随机种子以确保可重复性\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # 配置参数\n",
        "    in_channels = 198  # 实际高光谱数据的波段数，需要根据具体数据集调整\n",
        "    patch_size = 9\n",
        "    batch_size = 512\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.0006\n",
        "    weight_decay = 1e-4\n",
        "    warmup_epochs = 5  # 学习率预热epochs\n",
        "    model_save_path = 'best_model.pth'\n",
        "\n",
        "    # 检查CUDA可用性\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 加载数据集\n",
        "    try:\n",
        "        train_dataset = HSIChangeDetectionDataset(\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "            patch_size=patch_size,\n",
        "            mode='train'\n",
        "        )\n",
        "\n",
        "        val_dataset = HSIChangeDetectionDataset(\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "            patch_size=patch_size,\n",
        "            mode='val'\n",
        "        )\n",
        "\n",
        "        test_dataset = HSIChangeDetectionDataset(\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "            '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "            patch_size=patch_size,\n",
        "            mode='test'\n",
        "        )\n",
        "\n",
        "        # 检查数据集维度\n",
        "        sample = train_dataset[0]\n",
        "        in_channels = sample[0].shape[0]  # 动态获取输入通道数\n",
        "        print(f\"检测到输入通道数: {in_channels}\")\n",
        "        print(f\"数据样本形状: before={sample[0].shape}, after={sample[1].shape}, label={sample[2]}\")\n",
        "\n",
        "        # 创建数据加载器\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        print(f\"训练样本数: {len(train_dataset)}\")\n",
        "        print(f\"验证样本数: {len(val_dataset)}\")\n",
        "        print(f\"测试样本数: {len(test_dataset)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"加载数据集时出错: {e}\")\n",
        "        return\n",
        "\n",
        "    # 初始化模型\n",
        "    model = GLAFormer(in_channels=in_channels, dim=256, num_blocks=4, num_heads=8, patch_size=patch_size).to(device)\n",
        "    print(f\"模型初始化完成，参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # 动态设置类别权重 - 根据训练数据分布计算\n",
        "    neg_weight = 1.0\n",
        "    pos_weight = train_dataset.class_counts[0] / max(1, train_dataset.class_counts[1])  # 正样本权重\n",
        "    class_weights = torch.tensor([neg_weight, pos_weight]).to(device)\n",
        "    print(f\"类别权重: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # 优化器\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # 学习率调度器 - 使用预热和余弦退火\n",
        "    lr_scheduler = WarmupCosineScheduler(optimizer, warmup_epochs, num_epochs)\n",
        "\n",
        "    # 训练循环\n",
        "    best_val_f1 = 0.0\n",
        "    patience = 10  # 早停耐心值\n",
        "    counter = 0    # 早停计数器\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 更新学习率\n",
        "        lr_scheduler.step(epoch)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for before_patch, after_patch, labels in train_loader:\n",
        "            before_patch = before_patch.to(device)\n",
        "            after_patch = after_patch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(before_patch, after_patch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # 梯度裁剪，防止梯度爆炸\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * before_patch.size(0)\n",
        "            batch_count += 1\n",
        "\n",
        "            # 打印进度\n",
        "            if batch_count % 20 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_count}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # 验证阶段\n",
        "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, LR: {current_lr:.6f}')\n",
        "        print(f'  Train Loss: {train_loss:.4f}')\n",
        "        print(f'  Val Loss: {val_loss:.4f}, Acc: {val_metrics[\"accuracy\"]:.2f}%, F1: {val_metrics[\"f1\"]:.4f}, Kappa: {val_metrics[\"kappa\"]:.4f}')\n",
        "        print(f'  Val Confusion Matrix:\\n{val_metrics[\"confusion_matrix\"]}')\n",
        "\n",
        "        # 保存最佳模型（基于F1分数）\n",
        "        if val_metrics[\"f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1\"]\n",
        "            counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_f1': val_metrics[\"f1\"],\n",
        "                'val_metrics': val_metrics,\n",
        "            }, model_save_path)\n",
        "            print(f'  模型已保存: val_f1 从 {best_val_f1-val_metrics[\"f1\"]:.4f} 提升到 {val_metrics[\"f1\"]:.4f}')\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f'  F1未提升: {counter}/{patience}')\n",
        "\n",
        "        # 早停\n",
        "        if counter >= patience:\n",
        "            print(f'早停: 验证F1已经{patience}个epoch没有提升')\n",
        "            break\n",
        "\n",
        "    # 测试阶段\n",
        "    # 加载最佳模型\n",
        "    checkpoint = torch.load(model_save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"加载最佳模型（epoch {checkpoint['epoch']+1}，验证F1: {checkpoint['val_f1']:.4f}）\")\n",
        "\n",
        "    # 在测试集上评估\n",
        "    test_loss, test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\"\\n最终测试结果:\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
        "    print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Test Kappa: {test_metrics['kappa']:.4f}\")\n",
        "    print(f\"Test Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrA3to7aO-1I",
        "outputId": "e5ffcce9-6628-421e-abae-178d92a60f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "标签中的唯一值: [  0 255]\n",
            "类别分布 - 未变化: 96694, 变化: 9321\n",
            "标签中的唯一值: [  0 255]\n",
            "类别分布 - 未变化: 96694, 变化: 9321\n",
            "标签中的唯一值: [  0 255]\n",
            "类别分布 - 未变化: 96694, 变化: 9321\n",
            "检测到输入通道数: 198\n",
            "数据样本形状: before=torch.Size([198, 9, 9]), after=torch.Size([198, 9, 9]), label=0\n",
            "训练样本数: 74210\n",
            "验证样本数: 15902\n",
            "测试样本数: 15903\n",
            "模型初始化完成，参数量: 21,816,258\n",
            "类别权重: [ 1.       10.373779]\n",
            "Epoch 1/50, Batch 20/145, Loss: 0.6225\n",
            "Epoch 1/50, Batch 40/145, Loss: 0.5410\n",
            "Epoch 1/50, Batch 60/145, Loss: 0.4960\n",
            "Epoch 1/50, Batch 80/145, Loss: 0.4805\n",
            "Epoch 1/50, Batch 100/145, Loss: 0.3898\n",
            "Epoch 1/50, Batch 120/145, Loss: 0.3750\n",
            "Epoch 1/50, Batch 140/145, Loss: 0.3522\n",
            "Epoch 1/50, LR: 0.000001\n",
            "  Train Loss: 0.4965\n",
            "  Val Loss: 0.3666, Acc: 88.86%, F1: 0.5631, Kappa: 0.5064\n",
            "  Val Confusion Matrix:\n",
            "[[12988  1525]\n",
            " [  247  1142]]\n",
            "  模型已保存: val_f1 从 0.0000 提升到 0.5631\n",
            "Epoch 2/50, Batch 20/145, Loss: 0.2178\n",
            "Epoch 2/50, Batch 40/145, Loss: 0.3044\n",
            "Epoch 2/50, Batch 60/145, Loss: 0.3355\n",
            "Epoch 2/50, Batch 80/145, Loss: 0.1663\n",
            "Epoch 2/50, Batch 100/145, Loss: 0.1229\n",
            "Epoch 2/50, Batch 120/145, Loss: 0.2626\n",
            "Epoch 2/50, Batch 140/145, Loss: 0.1597\n",
            "Epoch 2/50, LR: 0.000121\n",
            "  Train Loss: 0.2081\n",
            "  Val Loss: 0.2889, Acc: 94.95%, F1: 0.7349, Kappa: 0.7072\n",
            "  Val Confusion Matrix:\n",
            "[[13986   527]\n",
            " [  276  1113]]\n",
            "  模型已保存: val_f1 从 0.0000 提升到 0.7349\n",
            "Epoch 3/50, Batch 20/145, Loss: 0.1770\n",
            "Epoch 3/50, Batch 40/145, Loss: 0.1732\n",
            "Epoch 3/50, Batch 60/145, Loss: 0.1568\n",
            "Epoch 3/50, Batch 80/145, Loss: 0.1147\n",
            "Epoch 3/50, Batch 100/145, Loss: 0.2092\n",
            "Epoch 3/50, Batch 120/145, Loss: 0.1353\n",
            "Epoch 3/50, Batch 140/145, Loss: 0.1003\n",
            "Epoch 3/50, LR: 0.000241\n",
            "  Train Loss: 0.1494\n",
            "  Val Loss: 0.1208, Acc: 94.63%, F1: 0.7583, Kappa: 0.7297\n",
            "  Val Confusion Matrix:\n",
            "[[13708   805]\n",
            " [   49  1340]]\n",
            "  模型已保存: val_f1 从 0.0000 提升到 0.7583\n",
            "Epoch 4/50, Batch 20/145, Loss: 0.2641\n",
            "Epoch 4/50, Batch 40/145, Loss: 0.1527\n",
            "Epoch 4/50, Batch 60/145, Loss: 0.1252\n"
          ]
        }
      ]
    }
  ]
}
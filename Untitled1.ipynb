{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fyB7OX1wRxAzzGjMseVnthDF1TC4Oe-d",
      "authorship_tag": "ABX9TyPG2WS+JtIEJGwltzvwza9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caaszj/GLAFormer/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== 可调整参数集中区域 ======================\n",
        "# 数据路径\n",
        "DATA_PATHS = {\n",
        "    'before': '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "    'after': '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "    'gt': '/content/drive/MyDrive/dataset zuixin/groundtruth.mat'\n",
        "}\n",
        "\n",
        "# 数据集划分比例\n",
        "DATASET_SPLIT = {\n",
        "    'train_ratio': 0.03,   # 训练集比例 - 3%\n",
        "    'val_ratio': 0.02,     # 验证集比例 - 2%\n",
        "    'test_ratio': 0.95,    # 测试集比例 - 95%\n",
        "    'random_seed': 42      # 随机种子，确保可重复性\n",
        "}\n",
        "\n",
        "# 模型参数\n",
        "MODEL_PARAMS = {\n",
        "    'in_channels': 198,  # 实际高光谱数据的波段数，需要根据具体数据集调整\n",
        "    'dim': 256,          # 模型内部维度\n",
        "    'num_blocks': 4,     # GLAFormer Block数量\n",
        "    'num_heads': 8,      # 注意力头数量\n",
        "    'patch_size': 9      # 图像块大小\n",
        "}\n",
        "\n",
        "# 训练参数\n",
        "TRAIN_PARAMS = {\n",
        "    'batch_size': 512,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.0006,\n",
        "    'weight_decay': 1e-4,\n",
        "    'warmup_epochs': 5,  # 学习率预热epochs\n",
        "    'patience': 2,       # 早停耐心值\n",
        "    'model_save_path': 'best_model.pth'\n",
        "}\n",
        "\n",
        "# 评估参数\n",
        "EVAL_PARAMS = {\n",
        "    'thresholds': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],  # 评估不同阈值\n",
        "    'default_threshold': 0.5,  # 默认分类阈值\n",
        "    'debug': False       # 是否显示详细调试信息和可视化\n",
        "}\n",
        "# ====================== 参数区域结束 ======================\n",
        "\n",
        "# 数据预处理和加载\n",
        "class HSIChangeDetectionDataset(Dataset):\n",
        "    def __init__(self, before_path, after_path, gt_path, patch_size=9, mode='train'):\n",
        "        self.before = self.load_mat(before_path)  # (H, W, C)\n",
        "        self.after = self.load_mat(after_path)\n",
        "        self.gt = self.load_gt(gt_path)  # (H, W)\n",
        "\n",
        "        # 检查标签值\n",
        "        unique_labels = np.unique(self.gt)\n",
        "        print(f\"标签中的唯一值: {unique_labels}\")\n",
        "\n",
        "        # 归一化\n",
        "        self.before = self._normalize(self.before)\n",
        "        self.after = self._normalize(self.after)\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.half = patch_size // 2\n",
        "        self.mode = mode\n",
        "\n",
        "        # 生成有效位置索引\n",
        "        self.coords = self.get_valid_coords()\n",
        "\n",
        "        # 首先计算整个数据集的类别分布\n",
        "        full_class_counts = self._get_class_distribution(self.coords)\n",
        "        print(f\"整个数据集类别分布 - 未变化: {full_class_counts[0]}, 变化: {full_class_counts[1]}\")\n",
        "\n",
        "        # 划分训练/验证/测试集\n",
        "        np.random.seed(DATASET_SPLIT['random_seed'])  # 设置随机种子以确保可重复性\n",
        "        idx = np.random.permutation(len(self.coords))  # 随机打乱索引\n",
        "\n",
        "        # 计算各部分数据的索引\n",
        "        train_ratio = DATASET_SPLIT['train_ratio']\n",
        "        val_ratio = DATASET_SPLIT['val_ratio']\n",
        "\n",
        "        train_num = int(len(idx) * train_ratio)  # 训练集大小\n",
        "        val_num = int(len(idx) * val_ratio)  # 验证集大小\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.coords = self.coords[idx[:train_num]]  # 取前train_ratio作为训练集\n",
        "        elif mode == 'val':\n",
        "            self.coords = self.coords[idx[train_num:train_num + val_num]]  # 取接下来的val_ratio作为验证集\n",
        "        else:\n",
        "            self.coords = self.coords[idx[train_num + val_num:]]  # 剩下的作为测试集\n",
        "\n",
        "        # 计算当前划分后的类别分布\n",
        "        self.class_counts = self._get_class_distribution(self.coords)\n",
        "        print(f\"{mode}集大小: {len(self.coords)}, 类别分布 - 未变化: {self.class_counts[0]}, 变化: {self.class_counts[1]}\")\n",
        "\n",
        "    def _normalize(self, data):\n",
        "        \"\"\"安全的归一化函数，处理可能的零除问题\"\"\"\n",
        "        data_min = np.min(data)\n",
        "        data_max = np.max(data)\n",
        "        if data_max == data_min:\n",
        "            return np.ones_like(data) * 0.5  # 如果所有值相同，返回0.5而不是0\n",
        "        normalized = (data - data_min) / (data_max - data_min)\n",
        "        return np.clip(normalized, 0, 1)\n",
        "\n",
        "    def _get_class_distribution(self, coordinates):\n",
        "        \"\"\"计算变化/非变化类别的样本数量\"\"\"\n",
        "        class_counts = [0, 0]\n",
        "        for i, j in coordinates:\n",
        "            label = 1 if self.gt[i, j] == 255 else 0\n",
        "            class_counts[label] += 1\n",
        "        return class_counts\n",
        "\n",
        "    def load_mat(self, path):\n",
        "        mat = loadmat(path)\n",
        "        # 获取.mat文件中的第一个键，假设它是数据键\n",
        "        keys = [k for k in mat.keys() if not k.startswith('__')]\n",
        "        if 'river_before' in mat:\n",
        "            return mat['river_before'].astype(np.float32)\n",
        "        elif 'river_after' in mat:\n",
        "            return mat['river_after'].astype(np.float32)\n",
        "        elif len(keys) > 0:\n",
        "            return mat[keys[0]].astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"无法从{path}加载数据\")\n",
        "\n",
        "    def load_gt(self, path):\n",
        "        mat = loadmat(path)\n",
        "        keys = [k for k in mat.keys() if not k.startswith('__')]\n",
        "        if 'lakelabel_v1' in mat:\n",
        "            return mat['lakelabel_v1'].astype(np.int64)\n",
        "        elif len(keys) > 0:\n",
        "            return mat[keys[0]].astype(np.int64)\n",
        "        else:\n",
        "            raise ValueError(f\"无法从{path}加载标签\")\n",
        "\n",
        "    def get_valid_coords(self):\n",
        "        H, W = self.gt.shape\n",
        "        coords = []\n",
        "        for i in range(self.half, H-self.half):\n",
        "            for j in range(self.half, W-self.half):\n",
        "                if self.gt[i, j] == 0 or self.gt[i, j] == 255:  # 只处理有效像素\n",
        "                    coords.append((i, j))\n",
        "        return np.array(coords)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, j = self.coords[idx]\n",
        "        # 提取双时相patch\n",
        "        before_patch = self.before[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        after_patch = self.after[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        # 转为CHW格式\n",
        "        before_patch = torch.from_numpy(before_patch).permute(2,0,1).float()\n",
        "        after_patch = torch.from_numpy(after_patch).permute(2,0,1).float()\n",
        "        label = 1 if self.gt[i, j] == 255 else 0  # 确保标签为0或1\n",
        "\n",
        "        return before_patch, after_patch, label\n",
        "\n",
        "# GLAM模块实现\n",
        "class GLAM(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, window_size=3):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # 确保维度可以被头数整除\n",
        "        self.split_dim = dim // 2\n",
        "        self.num_heads_local = max(1, num_heads // 2)  # 至少1个头\n",
        "        # 确保head_dim_local是整数\n",
        "        self.head_dim_local = self.split_dim // self.num_heads_local\n",
        "        self.split_dim = self.head_dim_local * self.num_heads_local  # 重新计算以确保可整除\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Local分支\n",
        "        self.local_qkv = nn.Conv2d(self.split_dim, self.split_dim*3, kernel_size=1)\n",
        "\n",
        "        # Global分支\n",
        "        self.global_q = nn.Conv2d(self.split_dim, self.split_dim, kernel_size=1)\n",
        "        self.global_kv = nn.Conv2d(self.split_dim, self.split_dim*2, kernel_size=1)\n",
        "\n",
        "        # 确保投影层维度正确\n",
        "        self.proj = nn.Conv2d(self.split_dim*2, dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # 分割通道维度\n",
        "        x_split = torch.split(x, [self.split_dim, C - self.split_dim], dim=1)\n",
        "        x_local = x_split[0]\n",
        "        x_global = x_split[0] if len(x_split) == 1 else x_split[1]\n",
        "\n",
        "        # 保证x_global的维度与split_dim一致\n",
        "        if x_global.shape[1] != self.split_dim:\n",
        "            x_global = F.adaptive_avg_pool2d(x_global, (H, W))\n",
        "            x_global = F.interpolate(x_global, size=(H, W), mode='bilinear')\n",
        "            if x_global.shape[1] > self.split_dim:\n",
        "                x_global = x_global[:, :self.split_dim, :, :]\n",
        "            elif x_global.shape[1] < self.split_dim:\n",
        "                padding = self.split_dim - x_global.shape[1]\n",
        "                x_global = torch.cat([x_global, torch.zeros(B, padding, H, W, device=x.device)], dim=1)\n",
        "\n",
        "        # Local Attention\n",
        "        qkv = self.local_qkv(x_local)  # [B, 3*split_dim, H, W]\n",
        "        qkv = qkv.reshape(B, 3, self.num_heads_local, self.head_dim_local, H, W)\n",
        "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # [B, num_heads_local, head_dim_local, H, W]\n",
        "\n",
        "        # 重塑张量以计算注意力\n",
        "        q = q.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k = k.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        v = v.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "\n",
        "        # 计算注意力分数 - 修正维度顺序\n",
        "        attn = torch.matmul(q.transpose(-2, -1), k) / math.sqrt(self.head_dim_local)  # [B, num_heads_local, H*W, H*W]\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        # 应用注意力\n",
        "        local_out = torch.matmul(v, attn.transpose(-2, -1))  # [B, num_heads_local, head_dim_local, H*W]\n",
        "        local_out = local_out.reshape(B, self.split_dim, H, W)\n",
        "\n",
        "        # Global Attention\n",
        "        x_pool = F.avg_pool2d(x_global, self.window_size)\n",
        "        pool_H, pool_W = x_pool.shape[2], x_pool.shape[3]\n",
        "\n",
        "        kv = self.global_kv(x_pool)  # [B, 2*split_dim, pool_H, pool_W]\n",
        "        kv = kv.reshape(B, 2, self.num_heads_local, self.head_dim_local, pool_H, pool_W)\n",
        "        k_g, v_g = kv[:, 0], kv[:, 1]  # [B, num_heads_local, head_dim_local, pool_H, pool_W]\n",
        "\n",
        "        q_g = self.global_q(x_global)  # [B, split_dim, H, W]\n",
        "        q_g = q_g.reshape(B, self.num_heads_local, self.head_dim_local, H, W)\n",
        "\n",
        "        # 重塑张量以计算注意力\n",
        "        q_g = q_g.reshape(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k_g = k_g.reshape(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "        v_g = v_g.reshape(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_g = torch.matmul(q_g.transpose(-2, -1), k_g) / math.sqrt(self.head_dim_local)  # [B, num_heads_local, H*W, pool_H*pool_W]\n",
        "        attn_g = F.softmax(attn_g, dim=-1)\n",
        "\n",
        "        # 应用注意力\n",
        "        global_out = torch.matmul(v_g, attn_g.transpose(-2, -1))  # [B, num_heads_local, head_dim_local, H*W]\n",
        "        global_out = global_out.reshape(B, self.split_dim, H, W)\n",
        "\n",
        "        # 连接并投影\n",
        "        out = torch.cat([local_out, global_out], dim=1)\n",
        "        return self.proj(out)\n",
        "\n",
        "# CGFN模块实现\n",
        "class CGFN(nn.Module):\n",
        "    def __init__(self, dim, expansion=4):\n",
        "        super().__init__()\n",
        "        hidden_dim = dim * expansion\n",
        "\n",
        "        self.conv1 = nn.Conv2d(dim, hidden_dim, 1)\n",
        "        self.dwconv3 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, groups=hidden_dim)\n",
        "        self.dwconv5 = nn.Conv2d(hidden_dim, hidden_dim, 5, padding=2, groups=hidden_dim)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim*2, dim, 1)\n",
        "\n",
        "        self.gate1 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gate2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.dwconv3(x)\n",
        "        x2 = self.dwconv5(x)\n",
        "\n",
        "        g1 = self.gate1(x1)\n",
        "        g2 = self.gate2(x2)\n",
        "\n",
        "        x1 = x1 * g1 + x2\n",
        "        x2 = x2 * g2 + x1\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        return self.conv2(x)\n",
        "\n",
        "# 辅助模块\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super().__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# GLAFormer Block\n",
        "class GLAFormerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size=3):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.glam = GLAM(dim, num_heads, window_size)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.cgfn = CGFN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 注意这里使用LayerNorm，需要先调整维度(B,C,H,W)->(B,H,W,C)\n",
        "        B, C, H, W = x.shape\n",
        "        x_perm = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)  # 归一化后转回[B, C, H, W]\n",
        "        x = x + self.glam(x_norm)\n",
        "\n",
        "        x_perm = x.permute(0, 2, 3, 1)\n",
        "        x_norm = self.norm2(x_perm).permute(0, 3, 1, 2)\n",
        "        x = x + self.cgfn(x_norm)\n",
        "        return x\n",
        "\n",
        "# 完整模型\n",
        "class GLAFormer(nn.Module):\n",
        "    def __init__(self, in_channels=198, dim=256, num_blocks=4, num_heads=8, patch_size=9):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.dim = dim\n",
        "\n",
        "        # 动态计算中心位置，避免硬编码\n",
        "        self.register_buffer('center', torch.tensor(patch_size // 2, dtype=torch.long))\n",
        "\n",
        "        # 输入嵌入层\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, dim, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 主干网络\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[GLAFormerBlock(dim, num_heads) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        # 分类头\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(dim, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 2, kernel_size=1)  # 输出通道数为2\n",
        "        )\n",
        "\n",
        "        # 全局特征和中心特征提取\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # 检查输入\n",
        "        if x1.dim() != 4 or x2.dim() != 4:\n",
        "            raise ValueError(f\"输入维度不正确: x1={x1.shape}, x2={x2.shape}\")\n",
        "\n",
        "        # 双时相特征融合\n",
        "        x = torch.cat([x1, x2], dim=1)  # [B, 2*in_channels, patch_size, patch_size]\n",
        "        x = self.embed(x)               # [B, dim, patch_size, patch_size]\n",
        "        x = self.blocks(x)              # [B, dim, patch_size, patch_size]\n",
        "        x = self.conv_block(x)          # [B, 2, patch_size, patch_size]\n",
        "\n",
        "        # 提取全局特征\n",
        "        global_feat = self.global_pool(x).flatten(1)  # [B, 2]\n",
        "\n",
        "        # 提取中心特征 - 使用动态计算的中心位置\n",
        "        center = self.center.item()\n",
        "        center_feat = x[:, :, center, center]  # [B, 2]\n",
        "\n",
        "        # 特征融合（加权平均）\n",
        "        return 0.6 * global_feat + 0.4 * center_feat  # [B, 2]\n",
        "\n",
        "# 计算混淆矩阵和详细指标\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"计算混淆矩阵和详细性能指标\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # 计算各种指标\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # 计算Kappa系数\n",
        "    pe = ((tn + fp) * (tn + fn) + (fn + tp) * (fp + tp)) / ((tp + tn + fp + fn) ** 2)\n",
        "    kappa = (accuracy - pe) / (1 - pe) if (1 - pe) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'confusion_matrix': cm,\n",
        "        'accuracy': accuracy * 100,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'kappa': kappa\n",
        "    }\n",
        "\n",
        "def generate_full_prediction(model, before_data, after_data, patch_size, device, threshold=0.5, debug=True):\n",
        "    \"\"\"\n",
        "    生成整幅预测图\n",
        "    Args:\n",
        "        model: 训练好的模型\n",
        "        before_data: 时相1数据 [H, W, C]\n",
        "        after_data: 时相2数据 [H, W, C]\n",
        "        patch_size: patch大小\n",
        "        device: 计算设备\n",
        "        threshold: 预测类别的置信度阈值，可调整\n",
        "        debug: 是否打印调试信息和可视化中间结果\n",
        "    Returns:\n",
        "        full_prediction: 整幅预测图 [H, W]\n",
        "        confidence_map: 置信度图 [H, W]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    H, W, C = before_data.shape\n",
        "    half = patch_size // 2\n",
        "    full_prediction = np.zeros((H, W), dtype=np.uint8)\n",
        "    confidence_map = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "    # 首先对整个数据进行归一化处理\n",
        "    def normalize_data(data):\n",
        "        data_min = np.min(data)\n",
        "        data_max = np.max(data)\n",
        "        if debug:\n",
        "            print(f\"数据范围: min={data_min}, max={data_max}\")\n",
        "\n",
        "        if data_max == data_min:\n",
        "            return np.ones_like(data) * 0.5\n",
        "        normalized = (data - data_min) / (data_max - data_min)\n",
        "        return np.clip(normalized, 0, 1)\n",
        "\n",
        "    before_data_norm = normalize_data(before_data)\n",
        "    after_data_norm = normalize_data(after_data)\n",
        "\n",
        "    if debug:\n",
        "        # 可视化归一化前后的数据\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # 原始数据直方图\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.hist(before_data.flatten(), bins=50)\n",
        "        plt.title('Before Data (Original)')\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.hist(after_data.flatten(), bins=50)\n",
        "        plt.title('After Data (Original)')\n",
        "\n",
        "        # 归一化后数据直方图\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.hist(before_data_norm.flatten(), bins=50)\n",
        "        plt.title('Before Data (Normalized)')\n",
        "\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.hist(after_data_norm.flatten(), bins=50)\n",
        "        plt.title('After Data (Normalized)')\n",
        "\n",
        "        # 可视化归一化后的数据（从中间波段选择一个）\n",
        "        mid_band = C // 2\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.imshow(before_data_norm[:,:,mid_band], cmap='jet')\n",
        "        plt.title(f'Normalized Before (Band {mid_band})')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.imshow(after_data_norm[:,:,mid_band], cmap='jet')\n",
        "        plt.title(f'Normalized After (Band {mid_band})')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('data_normalization.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()  # 关闭图形而不显示\n",
        "\n",
        "    # 边缘填充处理\n",
        "    pad_width = [(half, half), (half, half), (0, 0)]\n",
        "    before_padded = np.pad(before_data_norm, pad_width, mode='reflect')\n",
        "    after_padded = np.pad(after_data_norm, pad_width, mode='reflect')\n",
        "\n",
        "    # 批处理以加速计算\n",
        "    batch_size = 64\n",
        "    coords = []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            coords.append((i, j))\n",
        "\n",
        "    # 记录计算过程中的softmax输出\n",
        "    all_probs = []\n",
        "\n",
        "    for batch_start in tqdm(range(0, len(coords), batch_size), desc=\"Generating Prediction\"):\n",
        "        batch_end = min(batch_start + batch_size, len(coords))\n",
        "        batch_coords = coords[batch_start:batch_end]\n",
        "\n",
        "        batch_before = []\n",
        "        batch_after = []\n",
        "\n",
        "        for i, j in batch_coords:\n",
        "            i_pad = i + half\n",
        "            j_pad = j + half\n",
        "\n",
        "            before_patch = before_padded[i_pad-half:i_pad+half+1, j_pad-half:j_pad+half+1, :]\n",
        "            after_patch = after_padded[i_pad-half:i_pad+half+1, j_pad-half:j_pad+half+1, :]\n",
        "\n",
        "            before_tensor = torch.from_numpy(before_patch).permute(2,0,1).float()\n",
        "            after_tensor = torch.from_numpy(after_patch).permute(2,0,1).float()\n",
        "\n",
        "            batch_before.append(before_tensor)\n",
        "            batch_after.append(after_tensor)\n",
        "\n",
        "        if batch_before:  # 确保不是空批次\n",
        "            batch_before_tensor = torch.stack(batch_before).to(device)\n",
        "            batch_after_tensor = torch.stack(batch_after).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(batch_before_tensor, batch_after_tensor)\n",
        "                # 使用softmax获取概率\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                change_probs = probs[:, 1].cpu().numpy()  # 变化类别的概率\n",
        "                all_probs.extend(change_probs)\n",
        "\n",
        "                # 使用阈值决定最终预测类别\n",
        "                predictions = (change_probs > threshold).astype(np.uint8)\n",
        "\n",
        "            # 更新预测结果和置信度图\n",
        "            for idx, (i, j) in enumerate(batch_coords):\n",
        "                full_prediction[i, j] = predictions[idx]\n",
        "                confidence_map[i, j] = change_probs[idx]\n",
        "\n",
        "    if debug:\n",
        "        # 分析概率分布\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # 置信度直方图\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.hist(all_probs, bins=50)\n",
        "        plt.title('Confidence Score Distribution')\n",
        "        plt.xlabel('Confidence Score')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # 置信度热力图\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(confidence_map, cmap='jet', vmin=0, vmax=1)\n",
        "        plt.title('Confidence Map')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "        # 预测结果\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(full_prediction, cmap='gray', vmin=0, vmax=1)\n",
        "        plt.title(f'Prediction (threshold={threshold})')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()  # 关闭图形而不显示\n",
        "\n",
        "        # 打印关键统计信息\n",
        "        print(f\"\\n预测分析:\")\n",
        "        print(f\"置信度范围: min={np.min(confidence_map):.4f}, max={np.max(confidence_map):.4f}\")\n",
        "        print(f\"平均置信度: {np.mean(confidence_map):.4f}\")\n",
        "        print(f\"中位数置信度: {np.median(confidence_map):.4f}\")\n",
        "        print(f\"变化像素比例（阈值{threshold}）: {np.mean(full_prediction)*100:.2f}%\")\n",
        "\n",
        "    return full_prediction, confidence_map\n",
        "\n",
        "# 评估函数\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for before_patch, after_patch, labels in data_loader:\n",
        "            before_patch = before_patch.to(device)\n",
        "            after_patch = after_patch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(before_patch, after_patch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * before_patch.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(data_loader.dataset)\n",
        "    metrics = compute_metrics(all_labels, all_preds)\n",
        "\n",
        "    return val_loss, metrics\n",
        "\n",
        "# 学习率预热和余弦退火调度器\n",
        "class WarmupCosineScheduler:\n",
        "    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.min_lr = min_lr\n",
        "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
        "\n",
        "    def step(self, epoch):\n",
        "        if epoch < self.warmup_epochs:\n",
        "            # 线性预热\n",
        "            lr_scale = epoch / self.warmup_epochs\n",
        "        else:\n",
        "            # 余弦退火\n",
        "            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
        "            lr_scale = max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "        for i, group in enumerate(self.optimizer.param_groups):\n",
        "            group['lr'] = self.base_lrs[i] * lr_scale + self.min_lr\n",
        "\n",
        "def load_full_data(before_path, after_path, gt_path):\n",
        "    \"\"\"加载完整数据集（用于预测和可视化）\"\"\"\n",
        "    before = loadmat(before_path)['river_before'].astype(np.float32)\n",
        "    after = loadmat(after_path)['river_after'].astype(np.float32)\n",
        "    gt = loadmat(gt_path)['lakelabel_v1'].astype(np.uint8)\n",
        "    return before, after, gt\n",
        "\n",
        "def adjust_gt(gt):\n",
        "    \"\"\"将标签转换为0/1格式（原标签中0表示未变化，255表示变化）\"\"\"\n",
        "    return np.where(gt == 255, 1, 0)\n",
        "\n",
        "def run_evaluation(model_path, data_paths=None):\n",
        "    \"\"\"\n",
        "    运行模型评估和可视化\n",
        "    Args:\n",
        "        model_path: 模型权重路径\n",
        "        data_paths: 包含数据路径的字典，需要包含'before', 'after', 'gt'键\n",
        "    \"\"\"\n",
        "    # 使用提供的数据路径或默认路径\n",
        "    if data_paths is None:\n",
        "        data_paths = DATA_PATHS\n",
        "\n",
        "    # 设置设备\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 加载数据\n",
        "    before_data, after_data, gt = load_full_data(\n",
        "        data_paths['before'],\n",
        "        data_paths['after'],\n",
        "        data_paths['gt']\n",
        "    )\n",
        "    print(f\"数据形状: before={before_data.shape}, after={after_data.shape}, gt={gt.shape}\")\n",
        "\n",
        "    # 初始化和加载模型\n",
        "    in_channels = before_data.shape[2]  # 动态获取通道数\n",
        "    model = GLAFormer(\n",
        "        in_channels=in_channels,\n",
        "        dim=MODEL_PARAMS['dim'],\n",
        "        num_blocks=MODEL_PARAMS['num_blocks'],\n",
        "        num_heads=MODEL_PARAMS['num_heads'],\n",
        "        patch_size=MODEL_PARAMS['patch_size']\n",
        "    ).to(device)\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"加载模型成功，来自epoch {checkpoint['epoch']+1}\")\n",
        "    print(f\"验证指标: F1={checkpoint['val_f1']:.4f}\")\n",
        "\n",
        "    # 生成预测图\n",
        "    print(\"生成完整预测图...\")\n",
        "    full_prediction, confidence_map = generate_full_prediction(\n",
        "        model,\n",
        "        before_data,\n",
        "        after_data,\n",
        "        patch_size=MODEL_PARAMS['patch_size'],\n",
        "        device=device,\n",
        "        threshold=EVAL_PARAMS['default_threshold'],\n",
        "        debug=EVAL_PARAMS['debug']\n",
        "    )\n",
        "\n",
        "    # 准备ground truth\n",
        "    gt_binary = adjust_gt(gt)\n",
        "\n",
        "    # 尝试不同阈值\n",
        "    print(\"\\n计算不同阈值下的性能指标...\")\n",
        "    thresholds = EVAL_PARAMS['thresholds']\n",
        "    results = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        pred = (confidence_map > thresh).astype(np.uint8)\n",
        "        accuracy = np.mean(gt_binary == pred)\n",
        "\n",
        "        # 计算混淆矩阵\n",
        "        cm = confusion_matrix(gt_binary.flatten(), pred.flatten())\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # 计算各种指标\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'threshold': thresh,\n",
        "            'accuracy': accuracy * 100,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        print(f\"阈值 {thresh:.1f}: 准确率={accuracy*100:.2f}%, F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
        "\n",
        "    # 找出F1最高的阈值\n",
        "    best_result = max(results, key=lambda x: x['f1'])\n",
        "    print(f\"\\n最佳阈值: {best_result['threshold']}, F1={best_result['f1']:.4f}\")\n",
        "\n",
        "    # 使用最佳阈值生成最终预测\n",
        "    best_threshold = best_result['threshold']\n",
        "    final_prediction = (confidence_map > best_threshold).astype(np.uint8)\n",
        "\n",
        "    # 提取模型名称，用于图像标题\n",
        "    model_name = os.path.basename(model_path).split('.')[0]\n",
        "    title_base = f\"GLAFormer - {model_name}\"\n",
        "\n",
        "    # 保存Confidence Map图\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(confidence_map, cmap='jet', vmin=0, vmax=1)\n",
        "    plt.colorbar(fraction=0.046, pad=0.04)\n",
        "    plt.title(f'Confidence Map\\n{title_base}')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confidence_map.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 生成 best_results.png\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(gt_binary, cmap='gray', vmin=0, vmax=1)\n",
        "    plt.title('Ground Truth')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(final_prediction, cmap='gray', vmin=0, vmax=1)\n",
        "    plt.title(f'Best Prediction\\nThreshold={best_threshold}, F1={best_result[\"f1\"]:.4f}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    final_diff = np.abs(gt_binary - final_prediction)\n",
        "    plt.imshow(final_diff, cmap='Reds', vmin=0, vmax=1)\n",
        "    plt.title('Difference Map (Red: Error)')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.suptitle(title_base, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('best_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()  # 关闭图形而不显示\n",
        "\n",
        "    # 返回最佳预测结果\n",
        "    return {\n",
        "        'best_threshold': best_threshold,\n",
        "        'best_f1': best_result['f1'],\n",
        "        'final_prediction': final_prediction,\n",
        "        'confidence_map': confidence_map\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # 设置随机种子以确保可重复性\n",
        "    torch.manual_seed(DATASET_SPLIT['random_seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(DATASET_SPLIT['random_seed'])\n",
        "    np.random.seed(DATASET_SPLIT['random_seed'])\n",
        "\n",
        "    # 检查CUDA可用性\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 加载数据集\n",
        "    try:\n",
        "        train_dataset = HSIChangeDetectionDataset(\n",
        "            DATA_PATHS['before'],\n",
        "            DATA_PATHS['after'],\n",
        "            DATA_PATHS['gt'],\n",
        "            patch_size=MODEL_PARAMS['patch_size'],\n",
        "            mode='train'\n",
        "        )\n",
        "\n",
        "        val_dataset = HSIChangeDetectionDataset(\n",
        "            DATA_PATHS['before'],\n",
        "            DATA_PATHS['after'],\n",
        "            DATA_PATHS['gt'],\n",
        "            patch_size=MODEL_PARAMS['patch_size'],\n",
        "            mode='val'\n",
        "        )\n",
        "\n",
        "        test_dataset = HSIChangeDetectionDataset(\n",
        "            DATA_PATHS['before'],\n",
        "            DATA_PATHS['after'],\n",
        "            DATA_PATHS['gt'],\n",
        "            patch_size=MODEL_PARAMS['patch_size'],\n",
        "            mode='test'\n",
        "        )\n",
        "\n",
        "        # 检查数据集维度\n",
        "        sample = train_dataset[0]\n",
        "        in_channels = sample[0].shape[0]  # 动态获取输入通道数\n",
        "        print(f\"检测到输入通道数: {in_channels}\")\n",
        "        print(f\"数据样本形状: before={sample[0].shape}, after={sample[1].shape}, label={sample[2]}\")\n",
        "\n",
        "        # 创建数据加载器\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=TRAIN_PARAMS['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=TRAIN_PARAMS['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=TRAIN_PARAMS['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        print(f\"训练样本数: {len(train_dataset)}\")\n",
        "        print(f\"验证样本数: {len(val_dataset)}\")\n",
        "        print(f\"测试样本数: {len(test_dataset)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"加载数据集时出错: {e}\")\n",
        "        return\n",
        "\n",
        "    # 初始化模型\n",
        "    model = GLAFormer(\n",
        "        in_channels=in_channels,\n",
        "        dim=MODEL_PARAMS['dim'],\n",
        "        num_blocks=MODEL_PARAMS['num_blocks'],\n",
        "        num_heads=MODEL_PARAMS['num_heads'],\n",
        "        patch_size=MODEL_PARAMS['patch_size']\n",
        "    ).to(device)\n",
        "    print(f\"模型初始化完成，参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # 动态设置类别权重 - 根据训练数据分布计算\n",
        "    neg_weight = 1.0\n",
        "    pos_weight = train_dataset.class_counts[0] / max(1, train_dataset.class_counts[1])  # 正样本权重\n",
        "    class_weights = torch.tensor([neg_weight, pos_weight]).to(device)\n",
        "    print(f\"类别权重: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # 优化器\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=TRAIN_PARAMS['learning_rate'],\n",
        "        weight_decay=TRAIN_PARAMS['weight_decay']\n",
        "    )\n",
        "\n",
        "    # 学习率调度器 - 使用预热和余弦退火\n",
        "    lr_scheduler = WarmupCosineScheduler(\n",
        "        optimizer,\n",
        "        TRAIN_PARAMS['warmup_epochs'],\n",
        "        TRAIN_PARAMS['num_epochs']\n",
        "    )\n",
        "\n",
        "    # 训练循环\n",
        "    best_val_f1 = 0.0\n",
        "    patience = TRAIN_PARAMS['patience']  # 早停耐心值\n",
        "    counter = 0    # 早停计数器\n",
        "\n",
        "    for epoch in range(TRAIN_PARAMS['num_epochs']):\n",
        "        # 更新学习率\n",
        "        lr_scheduler.step(epoch)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for before_patch, after_patch, labels in train_loader:\n",
        "            before_patch = before_patch.to(device)\n",
        "            after_patch = after_patch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(before_patch, after_patch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # 梯度裁剪，防止梯度爆炸\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * before_patch.size(0)\n",
        "            batch_count += 1\n",
        "\n",
        "            # 打印进度\n",
        "            if batch_count % 20 == 0:\n",
        "                print(f'Epoch {epoch+1}/{TRAIN_PARAMS[\"num_epochs\"]}, Batch {batch_count}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # 验证阶段\n",
        "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{TRAIN_PARAMS[\"num_epochs\"]}, LR: {current_lr:.6f}')\n",
        "        print(f'  Train Loss: {train_loss:.4f}')\n",
        "        print(f'  Val Loss: {val_loss:.4f}, Acc: {val_metrics[\"accuracy\"]:.2f}%, F1: {val_metrics[\"f1\"]:.4f}, Kappa: {val_metrics[\"kappa\"]:.4f}')\n",
        "        print(f'  Val Confusion Matrix:\\n{val_metrics[\"confusion_matrix\"]}')\n",
        "\n",
        "        # 保存最佳模型（基于F1分数）\n",
        "        if val_metrics[\"f1\"] > best_val_f1:\n",
        "            old_best_f1 = best_val_f1  # 保存旧的最佳值用于打印\n",
        "            best_val_f1 = val_metrics[\"f1\"]\n",
        "            counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_f1': val_metrics[\"f1\"],\n",
        "                'val_metrics': val_metrics,\n",
        "            }, TRAIN_PARAMS['model_save_path'])\n",
        "            print(f'  模型已保存: val_f1 从 {old_best_f1:.4f} 提升到 {val_metrics[\"f1\"]:.4f}')\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f'  F1未提升: {counter}/{patience}')\n",
        "\n",
        "        # 早停\n",
        "        if counter >= patience:\n",
        "            print(f'早停: 验证F1已经{patience}个epoch没有提升')\n",
        "            break\n",
        "\n",
        "    # 测试阶段\n",
        "    # 加载最佳模型\n",
        "    checkpoint = torch.load(TRAIN_PARAMS['model_save_path'], weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"加载最佳模型（epoch {checkpoint['epoch']+1}，验证F1: {checkpoint['val_f1']:.4f}）\")\n",
        "\n",
        "    # 在测试集上评估\n",
        "    test_loss, test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\"\\n最终测试结果:\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
        "    print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Test Kappa: {test_metrics['kappa']:.4f}\")\n",
        "    print(f\"Test Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
        "\n",
        "    # 在训练完成后执行评估\n",
        "    if os.path.exists(TRAIN_PARAMS['model_save_path']):\n",
        "        print(\"\\n开始评估和可视化...\")\n",
        "        run_evaluation(TRAIN_PARAMS['model_save_path'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    # 检查是否只运行评估\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--evaluate\":\n",
        "        print(\"仅运行评估和可视化...\")\n",
        "        model_path = sys.argv[2] if len(sys.argv) > 2 else TRAIN_PARAMS['model_save_path']\n",
        "\n",
        "        # 如果提供了数据路径，则更新数据路径\n",
        "        custom_data_paths = None\n",
        "        if len(sys.argv) > 3:\n",
        "            custom_data_paths = {\n",
        "                'before': sys.argv[3],\n",
        "                'after': sys.argv[4] if len(sys.argv) > 4 else DATA_PATHS['after'],\n",
        "                'gt': sys.argv[5] if len(sys.argv) > 5 else DATA_PATHS['gt']\n",
        "            }\n",
        "\n",
        "        run_evaluation(model_path, custom_data_paths)\n",
        "    else:\n",
        "        main()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zcc6rIyzC0e",
        "outputId": "26a7f7f3-6a48-4a98-8197-cde86e9f9887"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "标签中的唯一值: [  0 255]\n",
            "整个数据集类别分布 - 未变化: 96694, 变化: 9321\n",
            "train集大小: 3180, 类别分布 - 未变化: 2929, 变化: 251\n",
            "标签中的唯一值: [  0 255]\n",
            "整个数据集类别分布 - 未变化: 96694, 变化: 9321\n",
            "val集大小: 2120, 类别分布 - 未变化: 1925, 变化: 195\n",
            "标签中的唯一值: [  0 255]\n",
            "整个数据集类别分布 - 未变化: 96694, 变化: 9321\n",
            "test集大小: 100715, 类别分布 - 未变化: 91840, 变化: 8875\n",
            "检测到输入通道数: 198\n",
            "数据样本形状: before=torch.Size([198, 9, 9]), after=torch.Size([198, 9, 9]), label=0\n",
            "训练样本数: 3180\n",
            "验证样本数: 2120\n",
            "测试样本数: 100715\n",
            "模型初始化完成，参数量: 21,816,258\n",
            "类别权重: [ 1.       11.669323]\n",
            "Epoch 1/5, LR: 0.000001\n",
            "  Train Loss: 0.6859\n",
            "  Val Loss: 0.6864, Acc: 76.23%, F1: 0.0769, Kappa: -0.0469\n",
            "  Val Confusion Matrix:\n",
            "[[1595  330]\n",
            " [ 174   21]]\n",
            "  模型已保存: val_f1 从 0.0000 提升到 0.0769\n",
            "Epoch 2/5, LR: 0.000121\n",
            "  Train Loss: 0.4352\n",
            "  Val Loss: 0.6486, Acc: 42.74%, F1: 0.1982, Kappa: 0.0451\n",
            "  Val Confusion Matrix:\n",
            "[[ 756 1169]\n",
            " [  45  150]]\n",
            "  模型已保存: val_f1 从 0.0769 提升到 0.1982\n",
            "Epoch 3/5, LR: 0.000241\n",
            "  Train Loss: 0.2995\n",
            "  Val Loss: 1.5173, Acc: 90.80%, F1: 0.0000, Kappa: 0.0000\n",
            "  Val Confusion Matrix:\n",
            "[[1925    0]\n",
            " [ 195    0]]\n",
            "  F1未提升: 1/2\n",
            "Epoch 4/5, LR: 0.000361\n",
            "  Train Loss: 0.4051\n",
            "  Val Loss: 0.3986, Acc: 66.98%, F1: 0.3519, Kappa: 0.2368\n",
            "  Val Confusion Matrix:\n",
            "[[1230  695]\n",
            " [   5  190]]\n",
            "  模型已保存: val_f1 从 0.1982 提升到 0.3519\n",
            "Epoch 5/5, LR: 0.000481\n",
            "  Train Loss: 0.3041\n",
            "  Val Loss: 0.4126, Acc: 63.77%, F1: 0.3322, Kappa: 0.2118\n",
            "  Val Confusion Matrix:\n",
            "[[1161  764]\n",
            " [   4  191]]\n",
            "  F1未提升: 1/2\n",
            "加载最佳模型（epoch 4，验证F1: 0.3519）\n",
            "\n",
            "最终测试结果:\n",
            "Test Loss: 0.4013\n",
            "Test Accuracy: 68.96%\n",
            "Test Precision: 0.2152\n",
            "Test Recall: 0.9526\n",
            "Test F1 Score: 0.3510\n",
            "Test Kappa: 0.2421\n",
            "Test Confusion Matrix:\n",
            "[[61004 30836]\n",
            " [  421  8454]]\n",
            "\n",
            "开始评估和可视化...\n",
            "Using device: cuda\n",
            "数据形状: before=(463, 241, 198), after=(463, 241, 198), gt=(463, 241)\n",
            "加载模型成功，来自epoch 4\n",
            "验证指标: F1=0.3519\n",
            "生成完整预测图...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Prediction: 100%|██████████| 1744/1744 [02:36<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "计算不同阈值下的性能指标...\n",
            "阈值 0.1: 准确率=13.84%, F1=0.1679, Precision=0.0916, Recall=1.0000\n",
            "阈值 0.2: 准确率=35.11%, F1=0.2113, Precision=0.1181, Recall=0.9999\n",
            "阈值 0.3: 准确率=48.37%, F1=0.2515, Precision=0.1439, Recall=0.9979\n",
            "阈值 0.4: 准确率=60.69%, F1=0.3046, Precision=0.1800, Recall=0.9906\n",
            "阈值 0.5: 准确率=68.96%, F1=0.3479, Precision=0.2128, Recall=0.9528\n",
            "阈值 0.6: 准确率=81.73%, F1=0.4640, Precision=0.3114, Recall=0.9098\n",
            "阈值 0.7: 准确率=87.80%, F1=0.5391, Precision=0.4014, Recall=0.8209\n",
            "阈值 0.8: 准确率=93.89%, F1=0.6211, Precision=0.6737, Recall=0.5761\n",
            "阈值 0.9: 准确率=92.54%, F1=0.2484, Precision=0.9978, Recall=0.1419\n",
            "\n",
            "最佳阈值: 0.8, F1=0.6211\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1XYaBMX34eyeTbYMpz7r7Emk0gzLQ7Hcc",
      "authorship_tag": "ABX9TyNDxKFIsMlNTyKqNcoHOOcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caaszj/TF/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "# 数据预处理和加载\n",
        "class HSIChangeDetectionDataset(Dataset):\n",
        "    def __init__(self, before_path, after_path, gt_path, patch_size=9, mode='train'):\n",
        "        self.before = self.load_mat(before_path)  # (H, W, C)\n",
        "        self.after = self.load_mat(after_path)\n",
        "        self.gt = self.load_gt(gt_path)  # (H, W)\n",
        "\n",
        "        # 归一化\n",
        "        self.before = (self.before - self.before.min()) / (self.before.max() - self.before.min())\n",
        "        self.after = (self.after - self.after.min()) / (self.after.max() - self.after.min())\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.half = patch_size // 2\n",
        "        self.mode = mode\n",
        "\n",
        "        # 生成有效位置索引\n",
        "        self.coords = self.get_valid_coords()\n",
        "\n",
        "        # 划分训练/验证/测试集\n",
        "        np.random.seed(42)  # 设置随机种子以确保可重复性\n",
        "        idx = np.random.permutation(len(self.coords))  # 随机打乱索引\n",
        "        train_num = int(len(idx) * 0.7)  # 训练集占70%\n",
        "        val_num = int(len(idx) * 0.15)  # 验证集占15%\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.coords = self.coords[idx[:train_num]]  # 取前70%作为训练集\n",
        "        elif mode == 'val':\n",
        "            self.coords = self.coords[idx[train_num:train_num + val_num]]  # 取接下来的15%作为验证集\n",
        "        else:\n",
        "            self.coords = self.coords[idx[train_num + val_num:]]  # 剩下的15%作为测试集\n",
        "\n",
        "    def load_mat(self, path):\n",
        "        mat = loadmat(path)\n",
        "        if 'river_before' in path:\n",
        "            return mat['river_before'].astype(np.float32)\n",
        "        elif 'river_after' in path:\n",
        "            return mat['river_after'].astype(np.float32)\n",
        "\n",
        "    def load_gt(self, path):\n",
        "        mat = loadmat(path)\n",
        "        return mat['lakelabel_v1'].astype(np.int64)\n",
        "\n",
        "    def get_valid_coords(self):\n",
        "        H, W = self.gt.shape\n",
        "        coords = []\n",
        "        for i in range(self.half, H-self.half):\n",
        "            for j in range(self.half, W-self.half):\n",
        "                if self.gt[i, j] in [0, 255]:  # 只处理有效像素\n",
        "                    coords.append((i, j))\n",
        "        return np.array(coords)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, j = self.coords[idx]\n",
        "        # 提取双时相patch\n",
        "        before_patch = self.before[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        after_patch = self.after[i-self.half:i+self.half+1, j-self.half:j+self.half+1, :]\n",
        "        # 转为CHW格式\n",
        "        before_patch = torch.from_numpy(before_patch).permute(2,0,1).float()\n",
        "        after_patch = torch.from_numpy(after_patch).permute(2,0,1).float()\n",
        "        label = self.gt[i, j] // 255  # 将255映射为1\n",
        "\n",
        "        return before_patch, after_patch, label\n",
        "\n",
        "# GLAM模块实现\n",
        "\n",
        "class GLAM(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, window_size=3):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.split_dim = dim // 2  # 明确分割维度\n",
        "        self.num_heads_local = num_heads // 2\n",
        "        self.head_dim_local = self.split_dim // self.num_heads_local  # 正确计算头维度\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Local分支\n",
        "        self.local_qkv = nn.Conv2d(self.split_dim, self.split_dim*3, kernel_size=1)\n",
        "\n",
        "        # Global分支\n",
        "        self.global_q = nn.Conv2d(self.split_dim, self.split_dim, kernel_size=1)\n",
        "        self.global_kv = nn.Conv2d(self.split_dim, self.split_dim*2, kernel_size=1)\n",
        "\n",
        "        self.proj = nn.Conv2d(dim, dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x_local, x_global = torch.split(x, self.split_dim, dim=1)\n",
        "\n",
        "        # Local Attention\n",
        "        qkv = self.local_qkv(x_local)  # [B, 3*split_dim, H, W]\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=1)  # 各为split_dim\n",
        "        q = q.view(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k = k.view(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        v = v.view(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "\n",
        "        attn = (q.transpose(-2, -1) @ k) / math.sqrt(self.head_dim_local)\n",
        "        local_out = (v @ attn.softmax(dim=-1)).view(B, self.split_dim, H, W)\n",
        "\n",
        "        # Global Attention\n",
        "        x_pool = F.avg_pool2d(x_global, self.window_size)\n",
        "        pool_H, pool_W = x_pool.shape[2], x_pool.shape[3]\n",
        "        kv = self.global_kv(x_pool)\n",
        "        k_g, v_g = torch.chunk(kv, 2, dim=1)\n",
        "\n",
        "        q_g = self.global_q(x_global).view(B, self.num_heads_local, self.head_dim_local, H*W)\n",
        "        k_g = k_g.view(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "        v_g = v_g.view(B, self.num_heads_local, self.head_dim_local, pool_H*pool_W)\n",
        "\n",
        "        attn_g = (q_g.transpose(-2, -1) @ k_g) / math.sqrt(self.head_dim_local)\n",
        "        global_out = (v_g @ attn_g.softmax(dim=-1).transpose(-2, -1)).view(B, self.split_dim, H, W)\n",
        "\n",
        "        return self.proj(torch.cat([local_out, global_out], dim=1))\n",
        "\n",
        "# CGFN模块实现\n",
        "class CGFN(nn.Module):\n",
        "    def __init__(self, dim, expansion=4):\n",
        "        super().__init__()\n",
        "        hidden_dim = dim * expansion\n",
        "\n",
        "        self.conv1 = nn.Conv2d(dim, hidden_dim, 1)\n",
        "        self.dwconv3 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, groups=hidden_dim)\n",
        "        self.dwconv5 = nn.Conv2d(hidden_dim, hidden_dim, 5, padding=2, groups=hidden_dim)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim*2, dim, 1)\n",
        "\n",
        "        self.gate1 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gate2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.dwconv3(x)\n",
        "        x2 = self.dwconv5(x)\n",
        "\n",
        "        g1 = self.gate1(x1)\n",
        "        g2 = self.gate2(x2)\n",
        "\n",
        "        x1 = x1 * g1 + x2\n",
        "        x2 = x2 * g2 + x1\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        return self.conv2(x)\n",
        "\n",
        "# GLAFormer Block\n",
        "class GLAFormerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size=3):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.glam = GLAM(dim, num_heads, window_size)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.cgfn = CGFN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # GLAM\n",
        "        x = x + self.glam(self.norm1(x.permute(0,2,3,1)).permute(0,3,1,2))\n",
        "        # CGFN\n",
        "        x = x + self.cgfn(self.norm2(x.permute(0,2,3,1)).permute(0,3,1,2))\n",
        "        return x\n",
        "\n",
        "# 完整模型\n",
        "\n",
        "class GLAFormer(nn.Module):\n",
        "    def __init__(self, in_channels=198, dim=256, num_blocks=4, num_heads=8, patch_size=9):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.center = patch_size // 2  # 预计算中心位置索引\n",
        "\n",
        "        # 输入嵌入层\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, dim, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 主干网络\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[GLAFormerBlock(dim, num_heads) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        # 修正后的分类头\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(dim, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 2, kernel_size=1)  # 输出通道数为2\n",
        "        )\n",
        "        self.global_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.center_extract = LambdaLayer(lambda x: x[:, :, self.center, self.center])\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # 双时相特征融合\n",
        "        x = torch.cat([x1, x2], dim=1)  # [B, 396, 9, 9]\n",
        "        x = self.embed(x)               # [B, 256, 9, 9]\n",
        "        x = self.blocks(x)              # [B, 256, 9, 9]\n",
        "\n",
        "        x = self.conv_block(x)          # [B, 2, 9, 9]\n",
        "\n",
        "        # 并行处理两种特征提取方式\n",
        "        global_feat = self.global_pool(x)  # [B, 2]\n",
        "        center_feat = self.center_extract(x)  # [B, 2]\n",
        "\n",
        "        # 特征融合（加权平均）\n",
        "        return 0.6 * global_feat + 0.4 * center_feat  # [B, 2]\n",
        "\n",
        "# 辅助模块\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super().__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# 训练配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GLAFormer().to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]).to(device))  # 处理类别不平衡\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0006)\n",
        "\n",
        "# 数据加载\n",
        "\n",
        "train_dataset = HSIChangeDetectionDataset(\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "    mode='train'\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "val_dataset = HSIChangeDetectionDataset(\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "    mode='val'\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "test_dataset = HSIChangeDetectionDataset(\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_before.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/river_after.mat',\n",
        "    '/content/drive/MyDrive/dataset zuixin/groundtruth.mat',\n",
        "    mode='test'\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 训练循环\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for before_patch, after_patch, labels in train_loader:\n",
        "        before_patch = before_patch.to(device)\n",
        "        after_patch = after_patch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(before_patch, after_patch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * before_patch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # 验证阶段\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for before_patch, after_patch, labels in val_loader:\n",
        "            before_patch = before_patch.to(device)\n",
        "            after_patch = after_patch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(before_patch, after_patch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * before_patch.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    # 保存最佳模型\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# 测试阶段\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for before_patch, after_patch, labels in test_loader:\n",
        "        before_patch = before_patch.to(device)\n",
        "        after_patch = after_patch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(before_patch, after_patch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * before_patch.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk9ai_OnRpM9",
        "outputId": "202d2628-cf16-4372-cc9c-0851b587f6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.2152, Val Loss: 0.2109, Val Acc: 85.08%\n",
            "Epoch 2/50, Train Loss: 0.1364, Val Loss: 0.1745, Val Acc: 90.22%\n"
          ]
        }
      ]
    }
  ]
}